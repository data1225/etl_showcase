{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a99a27-5550-4425-af46-3b69938fcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# 男頻高流量權謀爽劇演進研究：資料清理與產出報表\n",
    "#################################################################\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from path_setup import setup_project_root\n",
    "root = setup_project_root()\n",
    "\n",
    "import openpyxl, os, json, inspect, hdbscan, warnings, torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from umap import UMAP\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Local modules\n",
    "from etl_showcase.infrastructure.utils.file_utils import (\n",
    "    save_large_dataframe_to_excel,\n",
    "    read_and_combine_excel_sheets,\n",
    ")\n",
    "from etl_showcase.infrastructure.cleaning.language_translator import LanguageTranslator\n",
    "from etl_showcase.infrastructure.cleaning.text_tokenizer import (\n",
    "    jieba_tokenizer\n",
    ")\n",
    "from etl_showcase.infrastructure.cleaning.text_cleaner import (\n",
    "    remove_non_chinese_and_noise,\n",
    "    remove_non_english_and_noise,\n",
    "    remove_urls,\n",
    "    remove_all_punctuation,\n",
    "    clean_text,\n",
    ")\n",
    "from etl_showcase.infrastructure.nlp.sentiment_analyzer import SentimentAnalyzer\n",
    "from etl_showcase.infrastructure.nlp.topic_utils import (\n",
    "    default_n_neighbors,\n",
    "    get_topic_coordinates,\n",
    "    transform_to_refined_topics_by_culture,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.html_export import (\n",
    "    save_plotly_html,\n",
    "    save_html,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.general_visualizer import (\n",
    "    visualize_heatmap,\n",
    "    visualize_four_quadrants_violin_plot,\n",
    "    visualize_radar_chart,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.topic_visualizer import (\n",
    "    generate_bubble_chart_html,\n",
    "    generate_topic_sentiment_html,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.wordcloud_visualizer import generate_word_clouds_html\n",
    "\n",
    "zh_stopwords_custom = [\n",
    "    # 作品名\n",
    "    '琅琊榜', '琅', '琊', '榜', '慶餘年', '贅婿', '雪中悍刀行', '雪中悍', '刀行', '藏海傳', '海傳',\n",
    "\n",
    "    # 高頻雜訊\n",
    "    '電視劇', '訂閱', '劇', '電影', '晚上', '整片', '點可觀', '帶', '龍', '盡', '真', '卻', '一句',\n",
    "    \n",
    "    # 情感/程度副詞 (嚴重污染主題名稱的通用評價詞)\n",
    "    '真的', '很', '太', '非常', '好', '較', '稍', '最', \n",
    "\n",
    "    # 核心結構詞/連接詞 (高頻助詞、介詞、連詞)\n",
    "    '的', '了', '著', '之', '地', '得', '和', '與', '或', '但', '而',\n",
    "    '但是', '而且', '所以', '因為', '雖然', '然後', '就是', '將', '其', '則',\n",
    "    '是', '不是', '為', '給', '被', '在', '也',\n",
    "\n",
    "    # 代詞/指示詞 (人稱、指示、疑問)\n",
    "    '我', '他', '她', '你', '它', '們', '這', '那', '個',\n",
    "    '哪', '哪裡', '些', '某些', '每', '全部', '那樣', '這樣', '那麼', '什麼', '還是',\n",
    "    '還有', '這個', '那個', '這部', '那部',\n",
    "\n",
    "    # 通用動詞/狀態詞/否定詞\n",
    "    '不', '沒', '沒有', '都', '對', '又', '要', '能', '來', '把', '讓',\n",
    "    '還', '有', '只', '看', '說', '追',\n",
    "\n",
    "    # 時間/方位詞\n",
    "    '從', '到', '由', '於', '以', '上', '下', '中', '前', '後', '已經',\n",
    "\n",
    "    # 數量詞/語氣詞\n",
    "    '一', '二', '三', '一部', '啊', '吧', '嗎', '呢', '嘛', '呀',\n",
    "]\n",
    "\n",
    "\n",
    "# --- 核心修正函式 ---\n",
    "def get_BERTopic_model(culture: str, min_df_safe: int = 3) -> BERTopic:\n",
    "    \"\"\"\n",
    "    獲取設定優化後的 BERTopic 模型。\n",
    "    \n",
    "    Args:\n",
    "        culture (str): 'zh' 或 'en'。\n",
    "        min_df_safe (int): 預設為 3，用來過濾掉極端雜訊（單次或兩次出現的字元），避免 ValueError。\n",
    "    \"\"\"\n",
    "    \n",
    "    # UMAP 模型設定\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=default_n_neighbors(),  \n",
    "        n_components=2,  # 建議保留 2 以便視覺化\n",
    "        min_dist=0.0,  \n",
    "        metric='cosine',  \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    if culture == 'zh':\n",
    "        # 中文詞彙向量化模型設定\n",
    "        embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "        \n",
    "        # 將自訂停用詞加入到 CountVectorizer 中\n",
    "        vectorizer_model = CountVectorizer(\n",
    "            tokenizer=jieba_tokenizer, \n",
    "            min_df=min_df_safe,\n",
    "            max_df=0.75,\n",
    "            stop_words=zh_stopwords_custom\n",
    "        )\n",
    "        \n",
    "        return BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            language=\"chinese\",\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            umap_model=umap_model,\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "    else: \n",
    "        # 英文詞彙向量化模型設定\n",
    "        embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # 英文 CountVectorizer 設定\n",
    "        vectorizer_model = CountVectorizer(\n",
    "            stop_words='english', \n",
    "            min_df=min_df_safe, \n",
    "            max_df=0.75 \n",
    "        )\n",
    "        \n",
    "        return BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            language=\"english\",\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            umap_model=umap_model,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "# 資料清理完，偶爾還是會有奇怪符號或非純中文、英文的文本，但最後主題建模出來的主題。\n",
    "# 只有極少數會出現非純中文、英文的主題，考慮每個任務的CP值，該程式暫時先優化至此。\n",
    "def clean_and_translate(translator:LanguageTranslator, text: str, target_language: str):\n",
    "    detect_lang = translator.detect_lang(text)\n",
    "    if detect_lang is None:\n",
    "        return None  \n",
    "\n",
    "    if text== \"早上好，我真的很想看這些場景，所以我很高興，一如既往地謝謝您。\":\n",
    "        print(detect_lang)\n",
    "\n",
    "    if target_language == \"zh\":\n",
    "        target_language = \"zh-TW\"\n",
    "    elif target_language == \"en\":\n",
    "        target_language = \"en\"\n",
    "    else:\n",
    "        target_language = \"auto\"\n",
    "\n",
    "    if detect_lang.startswith('zh'):\n",
    "        detect_lang = \"zh-TW\"\n",
    "        text = remove_non_chinese_and_noise(text)\n",
    "    elif detect_lang.startswith('en'):\n",
    "        detect_lang = \"en\"\n",
    "        text = remove_urls(text)\n",
    "        text = remove_non_english_and_noise(text)\n",
    "    else:\n",
    "        detect_lang = \"auto\"\n",
    "        text = clean_text(text)\n",
    "        text = remove_all_punctuation(text)        \n",
    "        \n",
    "    if not detect_lang.startswith(target_language):\n",
    "        text = translator.translate(text=text, source_language=detect_lang, target_language=target_language)\n",
    "    if target_language.startswith('zh'):\n",
    "        text = translator.to_target_chinese_variant(\n",
    "            text=text, \n",
    "            target_variant=\"zh-TW\", \n",
    "        )\n",
    "        \n",
    "    # 清除翻譯完有雜訊的文本\n",
    "    if target_language.startswith('zh'):\n",
    "         text = remove_non_chinese_and_noise(text)\n",
    "    elif target_language.startswith('en'):\n",
    "         text = remove_non_english_and_noise(text)  \n",
    "\n",
    "    return text\n",
    "\n",
    "def load_data(file_path: str) -> (pd.DataFrame, Dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    載入主工作表和所有留言工作表的資料，並根據 Screenwork 欄位進行分組讀取。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Excel 檔案的路徑。\n",
    "\n",
    "    Returns:\n",
    "        (pd.DataFrame, Dict[str, pd.DataFrame]):\n",
    "        - main_df: 主工作表 '男頻高流量權謀爽劇影評影片資料' 的 DataFrame。\n",
    "        - comments_data: 字典，鍵為 Screenwork 名稱，值為對應的留言 DataFrame。\n",
    "    \"\"\"\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    main_df = pd.read_excel(xls, '男頻高流量權謀爽劇影評影片資料')\n",
    "    \n",
    "    comments_data = {}\n",
    "    \n",
    "    # 從 main_df 讀取所有不重複的 Screenwork 名稱\n",
    "    if 'Screenwork' in main_df.columns:\n",
    "        screenworks = main_df['Screenwork'].unique()\n",
    "    else:\n",
    "        print(\"主工作表中找不到 'Screenwork' 欄位。\")\n",
    "        return main_df, {}\n",
    "\n",
    "    # 根據 Screenwork 名稱尋找並讀取留言工作表\n",
    "    matching_sheets = []\n",
    "    for screenwork_name in screenworks:\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            if screenwork_name in sheet_name:\n",
    "                matching_sheets.append(sheet_name)\n",
    "        \n",
    "    # 遍歷符合條件的工作表並讀取資料\n",
    "    for sheet_name in matching_sheets:\n",
    "        # 提取劇作標題，並替換 '留言' 和 'reviews'\n",
    "        drama_title = sheet_name.replace('留言', '').replace('reviews', '').strip()\n",
    "        \n",
    "        # 將工作表資料存入字典，以處理同一個劇作有多個留言工作表的情況\n",
    "        if drama_title not in comments_data:\n",
    "            comments_data[drama_title] = pd.read_excel(xls, sheet_name)\n",
    "        else:\n",
    "            # 如果該劇作已存在，將新的留言資料附加到現有的 DataFrame\n",
    "            new_data = pd.read_excel(xls, sheet_name)\n",
    "            comments_data[drama_title] = pd.concat([comments_data[drama_title], new_data], ignore_index=True)\n",
    "                \n",
    "    return main_df, comments_data \n",
    "\n",
    "def preprocess_and_merge_data(main_df: pd.DataFrame, comments_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    預處理資料並將影片內容和留言合併。\n",
    "    - 修復了在計算留言按讚數時，因 Video ID 不匹配而導致的 IndexError。\n",
    "    \"\"\"\n",
    "    print('Start to preprocess and merge data')\n",
    "    \n",
    "    # 設定資料上限（每個文化領域的上限）\n",
    "    MAX_RECORDS = 5000\n",
    "\n",
    "    # -------------------\n",
    "    # 類型統一：確保 Video ID 都是字串，以便於跨資料表匹配\n",
    "    # -------------------\n",
    "    main_df['Video ID'] = main_df['Video ID'].astype(str)\n",
    "    for screenwork in comments_data:\n",
    "        if not comments_data[screenwork].empty:\n",
    "            comments_data[screenwork]['Video ID'] = comments_data[screenwork]['Video ID'].astype(str)\n",
    "\n",
    "    # -------------------\n",
    "    # 第一步：預估每個文化領域的總資料量\n",
    "    # -------------------\n",
    "    total_likes_by_culture = {}\n",
    "    \n",
    "    # 預估影片資料的總按讚數\n",
    "    main_df_likes = main_df.groupby('Cultural sphere')['Like count'].sum()\n",
    "    for culture, likes in main_df_likes.items():\n",
    "        total_likes_by_culture[culture] = total_likes_by_culture.get(culture, 0) + likes\n",
    "\n",
    "    # 預估留言資料的總按讚數\n",
    "    for screenwork, comments_df in comments_data.items():\n",
    "        if not comments_df.empty:\n",
    "            comments_df_likes = comments_df.groupby('Video ID')['Like count'].sum()\n",
    "            for video_id, likes in comments_df_likes.items():\n",
    "                \n",
    "                # 篩選主資料中匹配的行\n",
    "                matching_rows = main_df[main_df['Video ID'] == video_id]\n",
    "                if matching_rows.empty:\n",
    "                    continue\n",
    "                \n",
    "                # 找到對應的影片，取得其文化領域\n",
    "                cultural_sphere = matching_rows['Cultural sphere'].iloc[0]\n",
    "                total_likes_by_culture[cultural_sphere] = total_likes_by_culture.get(cultural_sphere, 0) + likes\n",
    "\n",
    "    print(f\"預估每個文化領域的總資料量 (按讚數總和): {total_likes_by_culture}\")\n",
    "    \n",
    "    # -------------------\n",
    "    # 第二步：根據總量限制調整每個文化領域的重複次數\n",
    "    # -------------------\n",
    "    scaling_factors = {}\n",
    "    for culture, total_likes in total_likes_by_culture.items():\n",
    "        if total_likes > MAX_RECORDS:\n",
    "            scaling_factor = MAX_RECORDS / total_likes\n",
    "            print(f\"文化領域 '{culture}' 總量超過 {MAX_RECORDS} 筆，將使用縮放比例: {scaling_factor:.4f}\")\n",
    "        else:\n",
    "            scaling_factor = 1\n",
    "            print(f\"文化領域 '{culture}' 總量未超過上限，不需要縮放。\")\n",
    "        scaling_factors[culture] = scaling_factor\n",
    "    \n",
    "    # -------------------\n",
    "    # 第三步：按比例分配與合併\n",
    "    # -------------------\n",
    "    translator = LanguageTranslator()\n",
    "    merged_data = []\n",
    "\n",
    "    for _, row in tqdm(main_df.iterrows(), total=len(main_df), desc=\"處理影片資料\"):\n",
    "        video_id = row['Video ID']\n",
    "        cultural_sphere = row['Cultural sphere']\n",
    "        screenwork = row['Screenwork']\n",
    "        scaling_factor = scaling_factors.get(cultural_sphere, 1) # 取得對應的縮放比例\n",
    "        doc = {\n",
    "            'Video ID': video_id,\n",
    "            'Publish year': row['Publish year'],\n",
    "            'Cultural sphere': cultural_sphere,\n",
    "            'Category': row['Category'],\n",
    "            'Screenwork': screenwork,\n",
    "            'text': '',\n",
    "        }\n",
    "        \n",
    "        ### 加入影片文本\n",
    "        video_title = str(row['Video title']) if not pd.isna(row['Video title']) else ''\n",
    "        video_description = str(row['Video description']) if not pd.isna(row['Video description']) else ''\n",
    "        video_content = str(row['Video content']) if not pd.isna(row['Video content']) else ''\n",
    "        video_content = video_content.replace('\\n', ' ').replace('\\r', '').replace('\\t', '')\n",
    "        video_text = video_title + ' ' + video_description + ' ' + video_content\n",
    "        \n",
    "        video_text = clean_and_translate(\n",
    "            translator=translator,\n",
    "            text=video_text,\n",
    "            target_language=cultural_sphere\n",
    "        )\n",
    "        if video_text is not None and video_text != '':\n",
    "            video_doc = doc.copy()\n",
    "            video_doc['text'] = video_text\n",
    "            \n",
    "            # 使用四捨五入計算重複次數\n",
    "            reps = round(row['Like count'] * scaling_factor)\n",
    "            merged_data.extend([video_doc] * reps)\n",
    "            \n",
    "        ### 加入留言文本\n",
    "        comments_df = comments_data.get(screenwork, pd.DataFrame())\n",
    "        if comments_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # 由於 Video ID 已經保證在 main_df 中（因為我們在迭代 main_df），\n",
    "        # 這裡只需要篩選留言中匹配的 Video ID 即可。\n",
    "        video_comments = comments_df[comments_df['Video ID'] == video_id]\n",
    "        \n",
    "        for _, comment_row in tqdm(video_comments.iterrows(), total=len(video_comments), leave=False, desc=f\"處理 {screenwork} 留言\"):\n",
    "            comment_text = str(comment_row['Text']) if not pd.isna(str(comment_row['Text'])) else ''\n",
    "            comment_text = clean_and_translate(\n",
    "                translator=translator,\n",
    "                text=comment_text,\n",
    "                target_language=cultural_sphere\n",
    "            )\n",
    "            if comment_text is not None and comment_text != '':\n",
    "                comment_doc = doc.copy()\n",
    "                comment_doc['text'] = comment_text\n",
    "\n",
    "                # 使用四捨五入計算重複次數\n",
    "                reps = round(comment_row['Like count'] * scaling_factor)\n",
    "                merged_data.extend([comment_doc] * reps)\n",
    "\n",
    "    print(f\"最終資料集大小: {len(merged_data)} 筆\")\n",
    "    return pd.DataFrame(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67d1b6-6ebc-45aa-9794-19168a68e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 預處理資料\n",
    "# 確保資料夾存在\n",
    "data_dirs_raw = os.path.join(os.getcwd(), 'data/raw')\n",
    "data_dirs_preprocessed = os.path.join(os.getcwd(), 'data/processed')\n",
    "os.makedirs(data_dirs_raw, exist_ok=True)\n",
    "os.makedirs(data_dirs_preprocessed, exist_ok=True)\n",
    "\n",
    "# 撈取所需資料，宣告共用變數\n",
    "data_file_path = os.path.join(os.getcwd(), data_dirs_raw, 'trickery_drama_evolution_study_data.xls')\n",
    "data_file_path_preprocessed = os.path.join(os.getcwd(), data_dirs_preprocessed, 'trickery_drama_evolution_study_data.xlsx')\n",
    "if not os.path.exists(data_file_path):\n",
    "    print(f\"Error: Data file not found at {data_file_path}\")\n",
    "    sys.exit()        \n",
    "main_df, comments_data = load_data(data_file_path)\n",
    "# 共用變數\n",
    "time_periods = {\n",
    "    '2015–2020': (2015, 2020),\n",
    "    '2021–2023': (2021, 2023),\n",
    "    '2024–2025': (2024, 2025)\n",
    "}\n",
    "cultures = ['zh', 'en']\n",
    "docs_dir = os.path.join(os.getcwd(), '..', 'docs/trickery_drama_evolution_study')\n",
    "os.makedirs(docs_dir, exist_ok=True)\n",
    "# 常見權謀元素 mapping 清單\n",
    "with open('../resources/trickery_element_mapping.json', 'r', encoding='utf-8') as f:\n",
    "    topic_mapping_list = json.load(f)\n",
    "\n",
    "if os.path.exists(data_file_path_preprocessed):\n",
    "    all_data_df = read_and_combine_excel_sheets(data_file_path_preprocessed)\n",
    "else:\n",
    "    # 將所有影片資料和留言資料合併並進行預處理\n",
    "    all_data_df = preprocess_and_merge_data(main_df, comments_data)\n",
    "    # 將資料寫入地端檔案，避免一直重複預處理大量文本\n",
    "    save_large_dataframe_to_excel(all_data_df, data_file_path_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dada1-055a-45a9-9c90-7fdbcfa70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 檢查預處理完之資料\n",
    "# Cultural sphere 分組\n",
    "counts_by_culture = all_data_df.groupby('Cultural sphere').size().reset_index(name='count')\n",
    "print(\"\\n=== 各文化圈筆數 ===\")\n",
    "print(counts_by_culture)\n",
    "\n",
    "# Cultural sphere + Publish year 分組\n",
    "counts_by_culture_year = (\n",
    "    all_data_df.groupby(['Cultural sphere', 'Publish year'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "print(\"\\n=== 各文化圈 + 年份筆數 ===\")\n",
    "print(counts_by_culture_year)\n",
    "\n",
    "# Screenwork 分組\n",
    "counts_by_screenwork = all_data_df.groupby('Screenwork').size().reset_index(name='count')\n",
    "print(\"\\n=== 各劇作筆數 ===\")\n",
    "print(counts_by_screenwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc6217-5e97-4c34-af69-5d364c427d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_surface_keywords_report(docs_dir, data, cultures, time_periods):\n",
    "    \"\"\"\n",
    "    根據文化區分，生成表層關鍵字文字雲報告，並移除停用詞及在過多文本中出現的詞語。\n",
    "    \"\"\"\n",
    "    print(\"Generating surface keywords report...\")\n",
    "    word_cloud_data = defaultdict(dict)\n",
    "\n",
    "    # 確保 'text' 欄位沒有 NaN 值\n",
    "    data['text'] = data['text'].fillna('')\n",
    "    \n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Publish year'] >= start) &\n",
    "                (data['Publish year'] <= end)\n",
    "            ]\n",
    "            \n",
    "            if not df_filtered.empty:\n",
    "                corpus = df_filtered['text'].tolist()\n",
    "                \n",
    "                if culture == 'zh':\n",
    "                    vectorizer = TfidfVectorizer(\n",
    "                        max_features=100,\n",
    "                        tokenizer=jieba_tokenizer,\n",
    "                        stop_words=zh_stopwords_custom,\n",
    "                        max_df=0.75\n",
    "                    )\n",
    "                else:\n",
    "                    vectorizer = TfidfVectorizer(\n",
    "                        max_features=100,\n",
    "                        stop_words='english',\n",
    "                        max_df=0.75\n",
    "                    )\n",
    "                \n",
    "                try:\n",
    "                    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "                    feature_names = vectorizer.get_feature_names_out()\n",
    "                    \n",
    "                    # 計算每個關鍵字的總 TF-IDF 權重\n",
    "                    word_weights = {}\n",
    "                    for col in range(len(feature_names)):\n",
    "                        word_weights[feature_names[col]] = tfidf_matrix[:, col].sum()\n",
    "                    \n",
    "                    word_cloud_data[culture][period_name] = word_weights\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error processing {culture} - {period_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # 生成文字雲並儲存為 HTML\n",
    "    html_content = generate_word_clouds_html(word_cloud_data, cultures, time_periods.keys())\n",
    "    output_path = os.path.join(docs_dir, \"timing_comparison/surface_keywords/all_generation_cultural.html\")\n",
    "    save_html(html_content, output_path)\n",
    "    print(f\"Surface keywords report is saved.\")\n",
    "\n",
    "generate_surface_keywords_report(docs_dir, all_data_df, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb742705-8555-45b1-b088-88de3c0c3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deep_topics_report(docs_dir, data, topic_mapping_list, cultures, time_periods):\n",
    "    \"\"\"\n",
    "    生成深層權謀元素氣泡圖報告。\n",
    "    \"\"\"\n",
    "    print(\"Generating deep topics report...\")\n",
    "    \n",
    "    # 在一開始篩選並儲存所有需要的 DataFrame\n",
    "    filtered_dataframes = {}\n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered_copy = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Publish year'] >= start) &\n",
    "                (data['Publish year'] <= end)\n",
    "            ].copy()\n",
    "            filtered_dataframes[(culture, period_name)] = df_filtered_copy\n",
    "            \n",
    "    for culture in cultures:          \n",
    "        print(f\"Processing BERTopic for {culture}...\")\n",
    "        \n",
    "        # 將每個時段的資料合併後再建模\n",
    "        all_texts = []\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered = filtered_dataframes.get((culture, period_name))\n",
    "            if df_filtered is not None:\n",
    "                all_texts.extend(df_filtered['text'].tolist())\n",
    "\n",
    "        if not all_texts:\n",
    "            print(f\"No data for {culture}. Skipping.\")\n",
    "            continue\n",
    "        # 檢查文件數量是否足夠， UMAP 降維時樣本數量需大於預設值，否則會報錯\n",
    "        if len(all_texts) < default_n_neighbors():\n",
    "            warnings.warn(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(all_texts)} 個。已跳過 {culture} 的主題報告生成。\", UserWarning)\n",
    "            continue\n",
    "        \n",
    "        # 訓練模型，並從所有文本中萃取出主題集合。\n",
    "        topic_model = get_BERTopic_model(culture=culture)\n",
    "        _, _ = topic_model.fit_transform(all_texts)\n",
    "        # 從模型中取得主題的二維座標。\n",
    "        topic_coords = get_topic_coordinates(topic_model)\n",
    "        \n",
    "        # 精煉主題清單\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        refined_topics = transform_to_refined_topics_by_culture(topic_info, topic_mapping_list, culture)\n",
    "\n",
    "        # 將主題分組\n",
    "        bubble_chart_data = defaultdict(list)\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered = filtered_dataframes.get((culture, period_name))\n",
    "            if df_filtered is None or df_filtered.empty:\n",
    "                continue\n",
    "\n",
    "            texts_in_period = df_filtered['text'].tolist()\n",
    "            topics_in_period, _ = topic_model.transform(texts_in_period)\n",
    "            \n",
    "            topic_counts = pd.Series(topics_in_period).value_counts().to_dict()\n",
    "            \n",
    "            period_data = []\n",
    "            for topic_id, count in topic_counts.items():\n",
    "                if topic_id == -1:\n",
    "                    continue\n",
    "                topic_name = refined_topics.get(topic_id)\n",
    "                if topic_name is not None:\n",
    "                    keywords = [str(item[0]) for item in topic_model.get_topic(topic_id)] \n",
    "                    x_coord, y_coord = topic_coords.get(topic_id, (None, None))\n",
    "        \n",
    "                    period_data.append({\n",
    "                        'id': topic_id,\n",
    "                        'name': topic_name,\n",
    "                        'count': count,\n",
    "                        'keywords': ', '.join(keywords) ,\n",
    "                        'x': x_coord, \n",
    "                        'y': y_coord,\n",
    "                    })\n",
    "            bubble_chart_data[period_name] = period_data\n",
    "\n",
    "        html_content = generate_bubble_chart_html(\n",
    "            data = bubble_chart_data, \n",
    "            display_language = 'zh',\n",
    "        )\n",
    "        output_path = os.path.join(docs_dir, f\"timing_comparison/topics_bubble/{culture}/all_generation.html\")\n",
    "        save_html(html_content, output_path)\n",
    "        print(f\"Deep topics report is saved.\")\n",
    "\n",
    "generate_deep_topics_report(docs_dir, all_data_df, topic_mapping_list, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa9517-f1c1-4e02-8d80-7373ab6c30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiment_reports(docs_dir, data, topic_mapping, cultures, time_periods):\n",
    "    print(\"Generating sentiment reports...\")\n",
    "\n",
    "    analyzer = SentimentAnalyzer()\n",
    "\n",
    "    filtered_dataframes = {}\n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered_copy = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Publish year'] >= start) &\n",
    "                (data['Publish year'] <= end)\n",
    "            ].copy()\n",
    "            filtered_dataframes[(culture, period_name)] = df_filtered_copy\n",
    "\n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            print(f\"Processing BERTopic for {culture} - {period_name}...\")\n",
    "            \n",
    "            df_filtered_period = filtered_dataframes.get((culture, period_name))\n",
    "            if df_filtered_period is None or df_filtered_period.empty:\n",
    "                print(f\"No data for {culture} in {period_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            texts_for_period = df_filtered_period['text'].tolist()\n",
    "            if len(texts_for_period) < default_n_neighbors():\n",
    "                print(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(texts_for_period)} 個。已跳過 {culture} {period_name} 的主題報告生成。\")\n",
    "                continue\n",
    "\n",
    "            # 建立 BERTopic\n",
    "            topic_model = get_BERTopic_model(culture=culture)\n",
    "            topics_in_period, _ = topic_model.fit_transform(texts_for_period)\n",
    "            \n",
    "            # 取得主題資訊\n",
    "            topic_info = topic_model.get_topic_info()\n",
    "            topic_id_to_name = {row['Topic']: row['Name'] for _, row in topic_info.iterrows()}\n",
    "\n",
    "            # 取前 20 個主題 (不包含 -1)\n",
    "            sorted_topic_info = topic_info.sort_values('Count', ascending=False)\n",
    "            top_20_topics = sorted_topic_info[sorted_topic_info['Topic'] != -1].head(20)['Topic'].tolist()\n",
    "            \n",
    "            # 根據主題名稱分組文本\n",
    "            topic_texts = defaultdict(list)\n",
    "            for topic_id, text in zip(topics_in_period, texts_for_period):\n",
    "                if topic_id in top_20_topics:\n",
    "                    topic_name = topic_id_to_name.get(topic_id, f\"Topic {topic_id}\")\n",
    "                    topic_texts[topic_name].append(text)\n",
    "            \n",
    "            if not topic_texts:\n",
    "                print(f\"No topics found for {culture} in {period_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 這裡可以用 analyzer 計算情感\n",
    "            period_sentiment_data = analyzer.analyze_sentiment_by_topic(\n",
    "                topic_data=topic_texts,\n",
    "            )\n",
    "            html_content = generate_topic_sentiment_html(\n",
    "                sentiment_data=period_sentiment_data,\n",
    "                display_language='zh',\n",
    "            )\n",
    "            output_path = os.path.join(docs_dir, f\"timing_comparison/topic_sentiment/{culture}/{period_name.replace('–', '-')}.html\")\n",
    "            save_html(html_content, output_path)\n",
    "            print(f\"Sentiment report for {period_name}  is saved.\")\n",
    "\n",
    "generate_sentiment_reports(docs_dir, all_data_df, topic_mapping_list, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4828241-9dd7-4521-a7f2-4b61d432d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_drama_category_reports(docs_dir, data, topic_mapping_list, cultures, time_periods):\n",
    "    \"\"\"\n",
    "    生成復仇劇 vs 一般權謀爽劇比對報告。\n",
    "    \"\"\"\n",
    "    print(\"Generating drama category comparison reports...\")\n",
    "        \n",
    "    analyzer = SentimentAnalyzer()\n",
    "\n",
    "    for culture in cultures:\n",
    "        print(f\"Processing drama category reports for {culture}...\")\n",
    "\n",
    "        # 共用變數\n",
    "        trickery_elements = set[str]\n",
    "        # 熱力圖資料\n",
    "        heatmap_data = defaultdict(dict)\n",
    "        # 小提琴圖資料\n",
    "        violin_plot_data = defaultdict(dict)\n",
    "\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            for category in ['復仇劇', '一般權謀劇']:\n",
    "                df_filtered = data[\n",
    "                    (data['Cultural sphere'] == culture) &\n",
    "                    (data['Publish year'] >= start) &\n",
    "                    (data['Publish year'] <= end) &\n",
    "                    (data['Category'] == category)\n",
    "                ]\n",
    "                \n",
    "                if df_filtered.empty:\n",
    "                    print(f\"No data for {culture} in {period_name} and {category}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                texts_in_period = df_filtered['text'].tolist()\n",
    "                # 檢查文件數量是否足夠， UMAP 降維時樣本數量需大於預設值，否則會報錯\n",
    "                if len(texts_in_period) < default_n_neighbors():\n",
    "                    print(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(texts_in_period)} 個。已跳過 [{culture}][{period_name}][{category}] 的主題報告生成。\")\n",
    "                    continue       \n",
    "                    \n",
    "                # BERTopic for Heatmap\n",
    "                # 因這份報告會遇 max_df corresponds to < documents than min_df 的問題，故調低 min_df 設定。\n",
    "                topic_model = get_BERTopic_model(culture=culture,min_df_safe=1)\n",
    "                topics, _ = topic_model.fit_transform(texts_in_period)\n",
    "\n",
    "                # 精鍊主題清單\n",
    "                topic_info = topic_model.get_topic_info()\n",
    "                refined_topics = transform_to_refined_topics_by_culture(topic_info, topic_mapping_list, culture)\n",
    "                trickery_elements = trickery_elements.union(set(refined_topics.values()))\n",
    "\n",
    "                # 計算每部作品中權謀元素的比重\n",
    "                drama_names = df_filtered['Screenwork'].unique()\n",
    "                for drama_name in tqdm(drama_names, total=len(drama_names), leave=False, desc=f\"計算每部作品中權謀元素的比重\"):\n",
    "                    drama_texts = df_filtered[df_filtered['Screenwork'] == drama_name]['text'].tolist()\n",
    "                    drama_topics, _ = topic_model.transform(drama_texts)\n",
    "                    \n",
    "                    topic_counts = pd.Series(drama_topics).value_counts().to_dict()\n",
    "                    total_count = sum(topic_counts.values())\n",
    "                    \n",
    "                    drama_topic_weights = {}\n",
    "                    for topic_id, count in topic_counts.items():\n",
    "                        if topic_id == -1: continue\n",
    "                        topic_name = refined_topics.get(topic_id)\n",
    "                        if topic_name is not None:\n",
    "                            drama_topic_weights[topic_name] = count / total_count\n",
    "                    \n",
    "                    heatmap_data[drama_name].update(drama_topic_weights)\n",
    "                    \n",
    "                # Sentiment analysis for Violin Plot\n",
    "                sentiment_results = analyzer.analyze_sentiment(texts_in_period)\n",
    "                sentiment_scores = [res['score'] if res['label'] == 'positive' else -res['score'] for res in sentiment_results]\n",
    "                \n",
    "                violin_plot_data[period_name][category] = sentiment_scores\n",
    "        \n",
    "        # 生成熱力圖\n",
    "        if heatmap_data: \n",
    "            topic_mapping = topic_mapping_list.get(culture)\n",
    "            fig_heatmap = visualize_heatmap(heatmap_data, trickery_elements)\n",
    "            fig_heatmap.update_layout(title_text='議題熱力圖', xaxis_title='議題', yaxis_title='作品名稱')\n",
    "            output_path = os.path.join(docs_dir, f\"drama_category_comparison/topic_heatmap/{culture}/all_generation.html\")\n",
    "            save_plotly_html(fig_heatmap,output_path)\n",
    "            print(f\"Topic heatmap report is saved.\")\n",
    "\n",
    "        # 生成小提琴圖\n",
    "        if violin_plot_data:\n",
    "            fig_violin_plot = visualize_four_quadrants_violin_plot(violin_plot_data)\n",
    "            fig_violin_plot.update_layout(\n",
    "                title_text='復仇劇與一般權謀劇情緒波動',\n",
    "                yaxis_title='情緒分數',\n",
    "                violinmode='group'\n",
    "            )\n",
    "            output_path = os.path.join(docs_dir, f\"drama_category_comparison/violin_plot/{culture}/all_generation.html\")\n",
    "            save_plotly_html(fig_violin_plot,output_path)\n",
    "            print(f\"Violin plot report is saved.\")\n",
    "\n",
    "generate_drama_category_reports(docs_dir, all_data_df, topic_mapping_list, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86da40-763f-4d32-a85a-95a248191c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_per_drama_reports(docs_dir, data, topic_mapping_list, cultures):\n",
    "    \"\"\"\n",
    "    生成各劇雷達圖報告。\n",
    "    \"\"\"\n",
    "    print(\"Generating per-drama radar chart reports...\")\n",
    "    \n",
    "    for culture in cultures:\n",
    "        df_filtered = data[data['Cultural sphere'] == culture]\n",
    "        dramas = df_filtered['Screenwork'].unique()\n",
    "        texts_culture = df_filtered['text'].tolist()\n",
    "        # 檢查文件數量是否足夠， UMAP 降維時樣本數量需大於預設值，否則會報錯\n",
    "        if len(texts_culture) < default_n_neighbors():\n",
    "            print(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(texts_culture)} 個。已跳過 {culture} 的主題報告生成。\")\n",
    "            continue        \n",
    "        topic_model = get_BERTopic_model(culture=culture)\n",
    "        _ = topic_model.fit_transform(texts_culture)\n",
    "        \n",
    "        # 精鍊主題清單\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        refined_topics = transform_to_refined_topics_by_culture(topic_info, topic_mapping_list, culture)\n",
    "        \n",
    "        for drama_name in dramas:\n",
    "            print(f\"Processing radar chart for {drama_name}...\")\n",
    "            df_filtered = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Screenwork'] == drama_name)\n",
    "            ]\n",
    "            \n",
    "            if df_filtered.empty:\n",
    "                continue\n",
    "\n",
    "            drama_texts = df_filtered['text'].tolist()           \n",
    "            drama_topics, _ = topic_model.transform(drama_texts)\n",
    "\n",
    "            # 根據文本數排序，選擇前10個主題，再計算選出來的主題的總文本數 \n",
    "            top_num = 10\n",
    "            topic_counts = pd.Series(drama_topics).value_counts()         \n",
    "            top_topics = topic_counts[topic_counts.index != -1].head(top_num)\n",
    "            total_count = top_topics.sum()\n",
    "\n",
    "            topic_proportions = defaultdict(float)\n",
    "            for topic_id, count in top_topics.items():\n",
    "                refined_name = refined_topics.get(topic_id)\n",
    "                if refined_name is not None:\n",
    "                    topic_proportions[refined_name] += count / total_count\n",
    "\n",
    "            fig_radar = visualize_radar_chart(topic_proportions)\n",
    "            fig_radar.update_layout(\n",
    "                title=f'{drama_name} Top {top_num} 熱門議題'\n",
    "            )          \n",
    "            safe_drama_name = drama_name.replace(' ', '_').replace('?', '').replace(':', '_')\n",
    "            output_path = os.path.join(docs_dir, f\"drama_analysis/radar_chart/{culture}/{safe_drama_name}.html\")\n",
    "            save_plotly_html(fig_radar,output_path)\n",
    "            print(f\"Radar chart for {drama_name} is saved.\")\n",
    "generate_per_drama_reports(docs_dir, all_data_df, topic_mapping_list, cultures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57542dd0-3405-4c0a-9fb2-872cd87f82ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
