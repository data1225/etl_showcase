{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a99a27-5550-4425-af46-3b69938fcc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: C:\\My data\\0.change jobs\\data_science_practice\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from path_setup import setup_project_root\n",
    "root = setup_project_root()\n",
    "\n",
    "import openpyxl, os, json, inspect, hdbscan, warnings, torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from umap import UMAP\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Local modules\n",
    "from etl_showcase.infrastructure.utils.file_utils import (\n",
    "    save_large_dataframe_to_excel,\n",
    "    read_and_combine_excel_sheets,\n",
    ")\n",
    "from etl_showcase.infrastructure.cleaning.language_translator import LanguageTranslator\n",
    "from etl_showcase.infrastructure.cleaning.text_tokenizer import (\n",
    "    jieba_tokenizer\n",
    ")\n",
    "from etl_showcase.infrastructure.cleaning.text_cleaner import (\n",
    "    clean_text,\n",
    "    remove_non_chinese_and_noise,\n",
    "    remove_non_english_and_noise,\n",
    "    remove_all_punctuation,\n",
    ")\n",
    "from etl_showcase.infrastructure.nlp.sentiment_analyzer import SentimentAnalyzer\n",
    "from etl_showcase.infrastructure.nlp.topic_utils import (\n",
    "    default_n_neighbors,\n",
    "    get_topic_coordinates,\n",
    "    transform_to_refined_topics_by_culture,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.html_export import (\n",
    "    save_plotly_html,\n",
    "    save_html,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.general_visualizer import (\n",
    "    visualize_heatmap,\n",
    "    visualize_four_quadrants_violin_plot,\n",
    "    visualize_radar_chart,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.topic_visualizer import (\n",
    "    generate_bubble_chart_html,\n",
    "    generate_topic_sentiment_html,\n",
    ")\n",
    "from etl_showcase.infrastructure.reporting.wordcloud_visualizer import generate_word_clouds_html\n",
    "\n",
    "def get_BERTopic_model(culture:str):\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=default_n_neighbors(), \n",
    "        n_components=2, \n",
    "        min_dist=0.0, \n",
    "        metric='cosine', \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    if culture == 'zh':\n",
    "        embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "        vectorizer_model = CountVectorizer(tokenizer=jieba_tokenizer, min_df=5, max_df=0.85)   \n",
    "        return BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            language=\"chinese\",\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            umap_model=umap_model,\n",
    "            verbose=True,\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        vectorizer_model = CountVectorizer(stop_words='english', min_df=5, max_df=0.85)\n",
    "        return BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            language=\"english\",\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            umap_model=umap_model,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# 資料清理完，偶爾還是會有奇怪符號或非純中文、英文的文本，但最後主題建模出來的主題。\n",
    "# 只有極少數會出現非純中文、英文的主題，考慮每個任務的CP值，該程式暫時先優化至此。\n",
    "def clean_and_translate(translator:LanguageTranslator, text: str, target_language: str):\n",
    "    detect_lang = translator.detect_lang(text)\n",
    "    if detect_lang is None:\n",
    "        return None  \n",
    "\n",
    "    if target_language == \"zh\":\n",
    "        target_language = \"zh-TW\"\n",
    "    elif target_language == \"en\":\n",
    "        target_language = \"en\"\n",
    "    else:\n",
    "        target_language = \"auto\"\n",
    "        \n",
    "    if detect_lang.startswith('zh'):\n",
    "        detect_lang = \"zh-TW\"\n",
    "        text = remove_non_chinese_and_noise(text)\n",
    "    elif detect_lang.startswith('en'):\n",
    "        detect_lang = \"zh-TW\"\n",
    "        text = remove_non_english_and_noise(text)\n",
    "    else:\n",
    "        detect_lang = \"auto\"\n",
    "        text = remove_all_punctuation(text)        \n",
    "        \n",
    "    if not detect_lang.startswith(target_language):\n",
    "        text = translator.translate(text=text, source_language=detect_lang, target_language=target_language)\n",
    "    if target_language.startswith('zh'):\n",
    "        text = translator.to_target_chinese_variant(\n",
    "            text=text, \n",
    "            target_variant=\"zh-TW\", \n",
    "        )\n",
    "\n",
    "    return text\n",
    "\n",
    "def load_data(file_path: str) -> (pd.DataFrame, Dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    載入主工作表和所有留言工作表的資料，並根據 Screenwork 欄位進行分組讀取。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Excel 檔案的路徑。\n",
    "\n",
    "    Returns:\n",
    "        (pd.DataFrame, Dict[str, pd.DataFrame]):\n",
    "        - main_df: 主工作表 '男頻高流量權謀爽劇影評影片資料' 的 DataFrame。\n",
    "        - comments_data: 字典，鍵為 Screenwork 名稱，值為對應的留言 DataFrame。\n",
    "    \"\"\"\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    main_df = pd.read_excel(xls, '男頻高流量權謀爽劇影評影片資料')\n",
    "    \n",
    "    comments_data = {}\n",
    "    \n",
    "    # 從 main_df 讀取所有不重複的 Screenwork 名稱\n",
    "    if 'Screenwork' in main_df.columns:\n",
    "        screenworks = main_df['Screenwork'].unique()\n",
    "    else:\n",
    "        print(\"主工作表中找不到 'Screenwork' 欄位。\")\n",
    "        return main_df, {}\n",
    "\n",
    "    # 根據 Screenwork 名稱尋找並讀取留言工作表\n",
    "    matching_sheets = []\n",
    "    for screenwork_name in screenworks:\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            if screenwork_name in sheet_name:\n",
    "                matching_sheets.append(sheet_name)\n",
    "        \n",
    "    # 遍歷符合條件的工作表並讀取資料\n",
    "    for sheet_name in matching_sheets:\n",
    "        # 提取劇作標題，並替換 '留言' 和 'reviews'\n",
    "        drama_title = sheet_name.replace('留言', '').replace('reviews', '').strip()\n",
    "        \n",
    "        # 將工作表資料存入字典，以處理同一個劇作有多個留言工作表的情況\n",
    "        if drama_title not in comments_data:\n",
    "            comments_data[drama_title] = pd.read_excel(xls, sheet_name)\n",
    "        else:\n",
    "            # 如果該劇作已存在，將新的留言資料附加到現有的 DataFrame\n",
    "            new_data = pd.read_excel(xls, sheet_name)\n",
    "            comments_data[drama_title] = pd.concat([comments_data[drama_title], new_data], ignore_index=True)\n",
    "                \n",
    "    return main_df, comments_data \n",
    "\n",
    "def preprocess_and_merge_data(main_df: pd.DataFrame, comments_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    預處理資料並將影片內容和留言合併。\n",
    "    \"\"\"\n",
    "    print('Start to preprocess and merge data')\n",
    "    \n",
    "    # 設定資料上限（每個文化領域的上限）\n",
    "    MAX_RECORDS = 5000\n",
    "\n",
    "    # -------------------\n",
    "    # 第一步：預估每個文化領域的總資料量\n",
    "    # -------------------\n",
    "    total_likes_by_culture = {}\n",
    "    \n",
    "    # 預估影片資料的總按讚數\n",
    "    main_df_likes = main_df.groupby('Cultural sphere')['Like count'].sum()\n",
    "    for culture, likes in main_df_likes.items():\n",
    "        total_likes_by_culture[culture] = total_likes_by_culture.get(culture, 0) + likes\n",
    "\n",
    "    # 預估留言資料的總按讚數\n",
    "    for screenwork, comments_df in comments_data.items():\n",
    "        if not comments_df.empty:\n",
    "            comments_df_likes = comments_df.groupby('Video ID')['Like count'].sum()\n",
    "            for video_id, likes in comments_df_likes.items():\n",
    "                # 找到對應的影片，取得其文化領域\n",
    "                cultural_sphere = main_df[main_df['Video ID'] == video_id]['Cultural sphere'].iloc[0]\n",
    "                total_likes_by_culture[cultural_sphere] = total_likes_by_culture.get(cultural_sphere, 0) + likes\n",
    "\n",
    "    print(f\"預估每個文化領域的總資料量 (按讚數總和): {total_likes_by_culture}\")\n",
    "    \n",
    "    # -------------------\n",
    "    # 第二步：根據總量限制調整每個文化領域的重複次數\n",
    "    # -------------------\n",
    "    scaling_factors = {}\n",
    "    for culture, total_likes in total_likes_by_culture.items():\n",
    "        if total_likes > MAX_RECORDS:\n",
    "            scaling_factor = MAX_RECORDS / total_likes\n",
    "            print(f\"文化領域 '{culture}' 總量超過 {MAX_RECORDS} 筆，將使用縮放比例: {scaling_factor:.4f}\")\n",
    "        else:\n",
    "            scaling_factor = 1\n",
    "            print(f\"文化領域 '{culture}' 總量未超過上限，不需要縮放。\")\n",
    "        scaling_factors[culture] = scaling_factor\n",
    "    \n",
    "    # -------------------\n",
    "    # 第三步：按比例分配與合併\n",
    "    # -------------------\n",
    "    translator = LanguageTranslator()\n",
    "    merged_data = []\n",
    "\n",
    "    for _, row in tqdm(main_df.iterrows(), total=len(main_df), desc=\"處理影片資料\"):\n",
    "        video_id = row['Video ID']\n",
    "        cultural_sphere = row['Cultural sphere']\n",
    "        screenwork = row['Screenwork']\n",
    "        scaling_factor = scaling_factors.get(cultural_sphere, 1) # 取得對應的縮放比例\n",
    "        doc = {\n",
    "            'Video ID': video_id,\n",
    "            'Publish year': row['Publish year'],\n",
    "            'Cultural sphere': cultural_sphere,\n",
    "            'Category': row['Category'],\n",
    "            'Screenwork': screenwork,\n",
    "            'text': '',\n",
    "        }\n",
    "        \n",
    "        ### 加入影片文本\n",
    "        video_title = str(row['Video title']) if not pd.isna(row['Video title']) else ''\n",
    "        video_description = str(row['Video description']) if not pd.isna(row['Video description']) else ''\n",
    "        video_content = str(row['Video content']) if not pd.isna(row['Video content']) else ''\n",
    "        video_content = video_content.replace('\\n', ' ').replace('\\r', '').replace('\\t', '')\n",
    "        video_text = video_title + ' ' + video_description + ' ' + video_content\n",
    "        \n",
    "        video_text = clean_and_translate(\n",
    "            translator=translator,\n",
    "            text=video_text,\n",
    "            target_language=cultural_sphere\n",
    "        )\n",
    "        if video_text is not None and video_text != '':\n",
    "            video_doc = doc.copy()\n",
    "            video_doc['text'] = video_text\n",
    "            \n",
    "            # 使用四捨五入計算重複次數\n",
    "            reps = round(row['Like count'] * scaling_factor)\n",
    "            merged_data.extend([video_doc] * reps)\n",
    "            \n",
    "        ### 加入留言文本\n",
    "        comments_df = comments_data.get(screenwork, pd.DataFrame())\n",
    "        if comments_df.empty:\n",
    "            continue\n",
    "        \n",
    "        for _, comment_row in tqdm(comments_df.iterrows(), total=len(comments_df), leave=False, desc=f\"處理 {screenwork} 留言\"):\n",
    "            if comment_row['Video ID'] == video_id:\n",
    "                comment_text = str(comment_row['Text']) if not pd.isna(str(comment_row['Text'])) else ''\n",
    "                comment_text = clean_and_translate(\n",
    "                    translator=translator,\n",
    "                    text=comment_text,\n",
    "                    target_language=cultural_sphere\n",
    "                )\n",
    "                if comment_text is not None and comment_text != '':\n",
    "                    comment_doc = doc.copy()\n",
    "                    comment_doc['text'] = comment_text\n",
    "\n",
    "                    # 使用四捨五入計算重複次數\n",
    "                    reps = round(comment_row['Like count'] * scaling_factor)\n",
    "                    merged_data.extend([comment_doc] * reps)\n",
    "\n",
    "    print(f\"最終資料集大小: {len(merged_data)} 筆\")\n",
    "    return pd.DataFrame(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f67d1b6-6ebc-45aa-9794-19168a68e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在讀取檔案 './data/processed/trickery_drama_evolution_study_data.xlsx'，共包含 1 個工作表。\n",
      "已讀取工作表 'Sheet1'，包含 9443 行資料。\n",
      "所有工作表已成功合併，總行數為 9443。\n",
      "zh資料 4732 筆\n",
      "en資料 4711 筆\n"
     ]
    }
   ],
   "source": [
    "# 撈取或宣告所需資料\n",
    "# 影評影片內容及留言\n",
    "data_file_path = \"./data/raw/trickery_drama_evolution_study_data.xls\"\n",
    "data_file_path_preprocessed = './data/processed/trickery_drama_evolution_study_data.xlsx'\n",
    "if not os.path.exists(data_file_path):\n",
    "    print(f\"Error: Data file not found at {data_file_path}\")\n",
    "    sys.exit()        \n",
    "main_df, comments_data = load_data(data_file_path)\n",
    "# 共用變數\n",
    "time_periods = {\n",
    "    '2015–2020': (2015, 2020),\n",
    "    '2021–2023': (2021, 2023),\n",
    "    '2024–2025': (2024, 2025)\n",
    "}\n",
    "cultures = ['zh', 'en']\n",
    "docs_dir = os.path.join(os.getcwd(), '..', 'docs/trickery_drama_evolution_study')\n",
    "os.makedirs(docs_dir, exist_ok=True)\n",
    "# 常見權謀元素 mapping 清單\n",
    "with open('../resources/trickery_element_mapping.json', 'r', encoding='utf-8') as f:\n",
    "    topic_mapping_list = json.load(f)\n",
    "\n",
    "if os.path.exists(data_file_path_preprocessed):\n",
    "    all_data_df = read_and_combine_excel_sheets(data_file_path_preprocessed)\n",
    "else:\n",
    "    # 將所有影片資料和留言資料合併並進行預處理\n",
    "    all_data_df = preprocess_and_merge_data(main_df, comments_data)\n",
    "    # 將資料寫入csv 避免一直重複預處理大量文本\n",
    "    save_large_dataframe_to_excel(all_data_df, data_file_path_preprocessed)\n",
    "\n",
    "print(f'zh資料 {len(all_data_df[all_data_df['Cultural sphere'] == 'zh'])} 筆')\n",
    "print(f'en資料 {len(all_data_df[all_data_df['Cultural sphere'] == 'en'])} 筆')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8fc6217-5e97-4c34-af69-5d364c427d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\c-116\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating surface keywords report...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.508 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\timing_comparison/surface_keywords/all_generation_cultural.html\n",
      "Surface keywords report is saved.\n"
     ]
    }
   ],
   "source": [
    "def generate_surface_keywords_report(docs_dir, data, cultures, time_periods):\n",
    "    \"\"\"\n",
    "    根據文化區分，生成表層關鍵字文字雲報告，並移除停用詞及在過多文本中出現的詞語。\n",
    "    \"\"\"\n",
    "    print(\"Generating surface keywords report...\")\n",
    "    word_cloud_data = defaultdict(dict)\n",
    "\n",
    "    # 定義常用的中文停用詞，把作品名也加進去，以避免噪音，\n",
    "    chinese_stop_words = [\n",
    "        \"琅琊榜\",\"琅\",\"琊\",\"榜\",\"慶餘年\",\"贅婿\",\"雪中悍刀行\",\"藏海傳\",\"海傳\",\n",
    "        \"的\",\"了\",\"著\",\"之\",\"地\",\"得\",\"和\",\"與\",\"或\",\"但是\",\"而且\",\"所以\",\n",
    "        \"是\",\"在\",\"我\",\"他\",\"她\",\"你\",\"它\",\"們\",\"這\",\"那\",\"個\",\"不\",\"也\",\"都\",\"對\",\"而\",\"但\",\n",
    "        \"因為\",\"雖然\",\"然後\",\"從\",\"到\",\"由\",\"於\",\"為\",\"以\",\"上\",\"下\",\"中\",\"前\",\"後\",\n",
    "        \"哪\",\"哪裡\",\"一\",\"二\",\"三\",\"些\",\"某些\",\"每\",\"全部\",\n",
    "        \"啊\",\"真的\",\"很\",\"非常\",\"較\",\"稍\",\"只\",\"還\",\"有\",\"沒有\",\n",
    "        \"吧\",\"呢\",\"嘛\",\"呀\",\"看\",\"就是\",\"就\",\"這個\",\"來\",\"把\",\"讓\",\"被\",\"給\",\"將\",\"其\",\"則\",\n",
    "        \"那樣\",\"這樣\",\"那麼\",\"什麼\",\"還是\",\"已經\",\"能\",\"又\",\"要\",\"嗎\",\"還有\",\"說\",\"沒\",\n",
    "        \"說\",\"是\",\"不是\",\"這是\",\"為\",\"追\",\"一部\"\n",
    "    ]\n",
    "\n",
    "    # 確保 'text' 欄位沒有 NaN 值\n",
    "    data['text'] = data['text'].fillna('')\n",
    "    \n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Publish year'] >= start) &\n",
    "                (data['Publish year'] <= end)\n",
    "            ]\n",
    "            \n",
    "            if not df_filtered.empty:\n",
    "                corpus = df_filtered['text'].tolist()\n",
    "                \n",
    "                if culture == 'zh':\n",
    "                    vectorizer = TfidfVectorizer(\n",
    "                        max_features=100,\n",
    "                        tokenizer=jieba_tokenizer,\n",
    "                        stop_words=chinese_stop_words,\n",
    "                        max_df=0.95\n",
    "                    )\n",
    "                else:\n",
    "                    vectorizer = TfidfVectorizer(\n",
    "                        max_features=100,\n",
    "                        stop_words='english',\n",
    "                        max_df=0.95\n",
    "                    )\n",
    "                \n",
    "                try:\n",
    "                    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "                    feature_names = vectorizer.get_feature_names_out()\n",
    "                    \n",
    "                    # 計算每個關鍵字的總 TF-IDF 權重\n",
    "                    word_weights = {}\n",
    "                    for col in range(len(feature_names)):\n",
    "                        word_weights[feature_names[col]] = tfidf_matrix[:, col].sum()\n",
    "                    \n",
    "                    word_cloud_data[culture][period_name] = word_weights\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error processing {culture} - {period_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # 生成文字雲並儲存為 HTML\n",
    "    html_content = generate_word_clouds_html(word_cloud_data, cultures, time_periods.keys())\n",
    "    output_path = os.path.join(docs_dir, \"timing_comparison/surface_keywords/all_generation_cultural.html\")\n",
    "    save_html(html_content, output_path)\n",
    "    print(f\"Surface keywords report is saved.\")\n",
    "\n",
    "generate_surface_keywords_report(docs_dir, all_data_df, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb742705-8555-45b1-b088-88de3c0c3d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating deep topics report...\n",
      "Processing BERTopic for zh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:50:19,631 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3c9bca01fe4887ad942ca792a6978f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:51:49,974 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 13:51:49,974 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 13:52:11,893 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 13:52:11,893 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 13:52:11,972 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 13:52:11,972 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 13:52:12,999 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1723dd1ce7c84eeb9c428d5b98a2d943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:53:52,262 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 13:53:52,278 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 13:53:52,278 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 13:53:52,372 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\timing_comparison/topics_bubble/zh/all_generation.html\n",
      "Deep topics report is saved.\n",
      "Processing BERTopic for en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:53:55,640 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6aa7762c6c4c4093add99bd25c694b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:54:03,912 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 13:54:03,912 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 13:54:16,577 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 13:54:16,577 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 13:54:16,639 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 13:54:16,639 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 13:54:16,722 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae336e3d443442cbc3af658820994fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:54:24,621 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 13:54:24,637 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 13:54:24,637 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 13:54:24,736 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\timing_comparison/topics_bubble/en/all_generation.html\n",
      "Deep topics report is saved.\n"
     ]
    }
   ],
   "source": [
    "def generate_deep_topics_report(docs_dir, data, topic_mapping_list, cultures, time_periods):\n",
    "    \"\"\"\n",
    "    生成深層權謀元素氣泡圖報告。\n",
    "    \"\"\"\n",
    "    print(\"Generating deep topics report...\")\n",
    "    \n",
    "    # 在一開始篩選並儲存所有需要的 DataFrame\n",
    "    filtered_dataframes = {}\n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered_copy = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Publish year'] >= start) &\n",
    "                (data['Publish year'] <= end)\n",
    "            ].copy()\n",
    "            filtered_dataframes[(culture, period_name)] = df_filtered_copy\n",
    "            \n",
    "    for culture in cultures:\n",
    "        print(f\"Processing BERTopic for {culture}...\")\n",
    "        \n",
    "        # 將每個時段的資料合併後再建模\n",
    "        all_texts = []\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered = filtered_dataframes.get((culture, period_name))\n",
    "            if df_filtered is not None:\n",
    "                all_texts.extend(df_filtered['text'].tolist())\n",
    "\n",
    "        if not all_texts:\n",
    "            print(f\"No data for {culture}. Skipping.\")\n",
    "            continue\n",
    "        # 檢查文件數量是否足夠， UMAP 降維時樣本數量需大於預設值，否則會報錯\n",
    "        if len(all_texts) < default_n_neighbors():\n",
    "            warnings.warn(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(all_texts)} 個。已跳過 {culture} 的主題報告生成。\", UserWarning)\n",
    "            continue\n",
    "        \n",
    "        # 訓練模型，並從所有文本中萃取出主題集合。\n",
    "        topic_model = get_BERTopic_model(culture=culture)\n",
    "        _, _ = topic_model.fit_transform(all_texts)\n",
    "        # 從模型中取得主題的二維座標。\n",
    "        topic_coords = get_topic_coordinates(topic_model)\n",
    "        \n",
    "        # 精煉主題清單\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        refined_topics = transform_to_refined_topics_by_culture(topic_info, topic_mapping_list, culture)\n",
    "\n",
    "        # 將主題分組\n",
    "        bubble_chart_data = defaultdict(list)\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered = filtered_dataframes.get((culture, period_name))\n",
    "            if df_filtered is None or df_filtered.empty:\n",
    "                continue\n",
    "\n",
    "            texts_in_period = df_filtered['text'].tolist()\n",
    "            topics_in_period, _ = topic_model.transform(texts_in_period)\n",
    "            \n",
    "            topic_counts = pd.Series(topics_in_period).value_counts().to_dict()\n",
    "            \n",
    "            period_data = []\n",
    "            for topic_id, count in topic_counts.items():\n",
    "                if topic_id == -1:\n",
    "                    continue\n",
    "                topic_name = refined_topics.get(topic_id, f\"Topic {topic_id}\")\n",
    "                keywords = [str(item[0]) for item in topic_model.get_topic(topic_id)] \n",
    "                x_coord, y_coord = topic_coords.get(topic_id, (None, None))\n",
    "    \n",
    "                period_data.append({\n",
    "                    'id': topic_id,\n",
    "                    'name': topic_name,\n",
    "                    'count': count,\n",
    "                    'keywords': ', '.join(keywords) ,\n",
    "                    'x': x_coord, \n",
    "                    'y': y_coord,\n",
    "                })\n",
    "            bubble_chart_data[period_name] = period_data\n",
    "\n",
    "        html_content = generate_bubble_chart_html(bubble_chart_data)\n",
    "        output_path = os.path.join(docs_dir, f\"timing_comparison/topics_bubble/{culture}/all_generation.html\")\n",
    "        save_html(html_content, output_path)\n",
    "        print(f\"Deep topics report is saved.\")\n",
    "\n",
    "generate_deep_topics_report(docs_dir, all_data_df, topic_mapping_list, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dfa9517-f1c1-4e02-8d80-7373ab6c30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentiment reports...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型載入成功並已動態取得最大長度。\n",
      "Processing BERTopic for zh - 2015–2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:55:04,611 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52520a2943c84ba3af52ede385ff3acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:56:35,772 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 13:56:35,772 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 13:56:46,023 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 13:56:46,023 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 13:56:46,103 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 13:56:46,103 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 13:56:47,107 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\timing_comparison/topic_sentiment/zh/2015-2020.html\n",
      "Sentiment report for 2015–2020  is saved.\n",
      "Processing BERTopic for zh - 2021–2023...\n",
      "No data for zh in 2021–2023. Skipping.\n",
      "Processing BERTopic for zh - 2024–2025...\n",
      "No data for zh in 2024–2025. Skipping.\n",
      "Processing BERTopic for en - 2015–2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:57:12,430 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8b37c057664dfaa0579b8f25b3d14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:57:20,239 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 13:57:20,239 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 13:57:32,284 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 13:57:32,284 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 13:57:32,347 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 13:57:32,347 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 13:57:32,434 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\timing_comparison/topic_sentiment/en/2015-2020.html\n",
      "Sentiment report for 2015–2020  is saved.\n",
      "Processing BERTopic for en - 2021–2023...\n",
      "No data for en in 2021–2023. Skipping.\n",
      "Processing BERTopic for en - 2024–2025...\n",
      "No data for en in 2024–2025. Skipping.\n"
     ]
    }
   ],
   "source": [
    "def generate_sentiment_reports(docs_dir, data, topic_mapping, cultures, time_periods):\n",
    "    print(\"Generating sentiment reports...\")\n",
    "\n",
    "    analyzer = SentimentAnalyzer()\n",
    "\n",
    "    filtered_dataframes = {}\n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            df_filtered_copy = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Publish year'] >= start) &\n",
    "                (data['Publish year'] <= end)\n",
    "            ].copy()\n",
    "            filtered_dataframes[(culture, period_name)] = df_filtered_copy\n",
    "\n",
    "    for culture in cultures:\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            print(f\"Processing BERTopic for {culture} - {period_name}...\")\n",
    "            \n",
    "            df_filtered_period = filtered_dataframes.get((culture, period_name))\n",
    "            if df_filtered_period is None or df_filtered_period.empty:\n",
    "                print(f\"No data for {culture} in {period_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            texts_for_period = df_filtered_period['text'].tolist()\n",
    "            if len(texts_for_period) < default_n_neighbors():\n",
    "                print(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(texts_for_period)} 個。已跳過 {culture} {period_name} 的主題報告生成。\")\n",
    "                continue\n",
    "\n",
    "            # 建立 BERTopic\n",
    "            topic_model = get_BERTopic_model(culture=culture)\n",
    "            topics_in_period, _ = topic_model.fit_transform(texts_for_period)\n",
    "            \n",
    "            # 取得主題資訊\n",
    "            topic_info = topic_model.get_topic_info()\n",
    "            topic_id_to_name = {row['Topic']: row['Name'] for _, row in topic_info.iterrows()}\n",
    "\n",
    "            # 取前 20 個主題 (不包含 -1)\n",
    "            sorted_topic_info = topic_info.sort_values('Count', ascending=False)\n",
    "            top_20_topics = sorted_topic_info[sorted_topic_info['Topic'] != -1].head(20)['Topic'].tolist()\n",
    "            \n",
    "            # 根據主題名稱分組文本\n",
    "            topic_texts = defaultdict(list)\n",
    "            for topic_id, text in zip(topics_in_period, texts_for_period):\n",
    "                if topic_id in top_20_topics:\n",
    "                    topic_name = topic_id_to_name.get(topic_id, f\"Topic {topic_id}\")\n",
    "                    topic_texts[topic_name].append(text)\n",
    "            \n",
    "            if not topic_texts:\n",
    "                print(f\"No topics found for {culture} in {period_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 這裡可以用 analyzer 計算情感\n",
    "            period_sentiment_data = analyzer.analyze_sentiment_by_topic(\n",
    "                topic_data=topic_texts,\n",
    "            )\n",
    "            html_content = generate_topic_sentiment_html(\n",
    "                sentiment_data=period_sentiment_data,\n",
    "                display_language='zh',\n",
    "            )\n",
    "            output_path = os.path.join(docs_dir, f\"timing_comparison/topic_sentiment/{culture}/{period_name.replace('–', '-')}.html\")\n",
    "            save_html(html_content, output_path)\n",
    "            print(f\"Sentiment report for {period_name}  is saved.\")\n",
    "\n",
    "generate_sentiment_reports(docs_dir, all_data_df, topic_mapping_list, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4828241-9dd7-4521-a7f2-4b61d432d151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating drama category comparison reports...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型載入成功並已動態取得最大長度。\n",
      "Processing drama category reports for zh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 13:59:26,855 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555db591f3974341b04347e9af02cba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:00:05,306 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 14:00:05,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 14:00:13,603 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:00:13,603 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 14:00:13,634 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:00:13,650 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 14:00:14,123 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "計算每部作品中權謀元素的比重:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299a02e466d0467abbeee0c9b2989118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:00:55,907 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:00:55,907 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:00:55,907 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:00:55,954 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:02:36,082 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d76df43067c44558f9d5026319dbcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:03:29,746 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 14:03:29,747 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 14:03:41,011 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:03:41,012 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 14:03:41,055 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:03:41,058 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 14:03:41,617 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "計算每部作品中權謀元素的比重:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88e5db1dbcb41abbf22ba849277206f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:04:35,198 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:04:35,208 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:04:35,209 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:04:35,250 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for zh in 2021–2023 and 復仇劇. Skipping.\n",
      "No data for zh in 2021–2023 and 一般權謀劇. Skipping.\n",
      "No data for zh in 2024–2025 and 復仇劇. Skipping.\n",
      "No data for zh in 2024–2025 and 一般權謀劇. Skipping.\n",
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_category_comparison/topic_heatmap/zh/all_generation.html\n",
      "Topic heatmap report is saved.\n",
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_category_comparison/violin_plot/zh/all_generation.html\n",
      "Violin plot report is saved.\n",
      "Processing drama category reports for en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:05:57,244 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d371262be1d54a7bad02a39a79b1ce3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:06:01,001 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 14:06:01,002 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 14:06:08,726 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:06:08,727 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 14:06:08,757 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:06:08,760 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 14:06:08,798 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "計算每部作品中權謀元素的比重:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3562e061cfda4850a6c9bef992441773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:06:12,635 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:06:12,639 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:06:12,640 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:06:12,681 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:07:00,301 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351071e28ed441c7b8fb226b20422598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:07:05,304 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 14:07:05,304 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 14:07:20,412 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:07:20,413 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 14:07:20,455 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:07:20,458 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 14:07:20,497 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "計算每部作品中權謀元素的比重:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2892fb2dad4d46608b05eb79833f6c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:07:25,325 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:07:25,330 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:07:25,331 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:07:25,373 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for en in 2021–2023 and 復仇劇. Skipping.\n",
      "No data for en in 2021–2023 and 一般權謀劇. Skipping.\n",
      "No data for en in 2024–2025 and 復仇劇. Skipping.\n",
      "No data for en in 2024–2025 and 一般權謀劇. Skipping.\n",
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_category_comparison/topic_heatmap/en/all_generation.html\n",
      "Topic heatmap report is saved.\n",
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_category_comparison/violin_plot/en/all_generation.html\n",
      "Violin plot report is saved.\n"
     ]
    }
   ],
   "source": [
    "def generate_drama_category_reports(docs_dir, data, topic_mapping_list, cultures, time_periods):\n",
    "    \"\"\"\n",
    "    生成復仇劇 vs 一般權謀爽劇比對報告。\n",
    "    \"\"\"\n",
    "    print(\"Generating drama category comparison reports...\")\n",
    "        \n",
    "    analyzer = SentimentAnalyzer()\n",
    "\n",
    "    for culture in cultures:\n",
    "        print(f\"Processing drama category reports for {culture}...\")\n",
    "\n",
    "        # 共用變數\n",
    "        trickery_elements = set[str]\n",
    "        # 熱力圖資料\n",
    "        heatmap_data = defaultdict(dict)\n",
    "        # 小提琴圖資料\n",
    "        violin_plot_data = defaultdict(dict)\n",
    "\n",
    "        for period_name, (start, end) in time_periods.items():\n",
    "            for category in ['復仇劇', '一般權謀劇']:\n",
    "                df_filtered = data[\n",
    "                    (data['Cultural sphere'] == culture) &\n",
    "                    (data['Publish year'] >= start) &\n",
    "                    (data['Publish year'] <= end) &\n",
    "                    (data['Category'] == category)\n",
    "                ]\n",
    "                \n",
    "                if df_filtered.empty:\n",
    "                    print(f\"No data for {culture} in {period_name} and {category}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                texts_in_period = df_filtered['text'].tolist()\n",
    "                # 檢查文件數量是否足夠， UMAP 降維時樣本數量需大於預設值，否則會報錯\n",
    "                if len(texts_in_period) < default_n_neighbors():\n",
    "                    print(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(texts_in_period)} 個。已跳過 [{culture}][{period_name}][{category}] 的主題報告生成。\")\n",
    "                    continue       \n",
    "                    \n",
    "                # BERTopic for Heatmap\n",
    "                topic_model = get_BERTopic_model(culture=culture)\n",
    "                topics, _ = topic_model.fit_transform(texts_in_period)\n",
    "\n",
    "                # 精鍊主題清單\n",
    "                topic_info = topic_model.get_topic_info()\n",
    "                refined_topics = transform_to_refined_topics_by_culture(topic_info, topic_mapping_list, culture)\n",
    "                trickery_elements = trickery_elements.union(set(refined_topics.values()))\n",
    "\n",
    "                # 計算每部作品中權謀元素的比重\n",
    "                drama_names = df_filtered['Screenwork'].unique()\n",
    "                for drama_name in tqdm(drama_names, total=len(drama_names), leave=False, desc=f\"計算每部作品中權謀元素的比重\"):\n",
    "                    drama_texts = df_filtered[df_filtered['Screenwork'] == drama_name]['text'].tolist()\n",
    "                    drama_topics, _ = topic_model.transform(drama_texts)\n",
    "                    \n",
    "                    topic_counts = pd.Series(drama_topics).value_counts().to_dict()\n",
    "                    total_count = sum(topic_counts.values())\n",
    "                    \n",
    "                    drama_topic_weights = {}\n",
    "                    for topic_id, count in topic_counts.items():\n",
    "                        if topic_id == -1: continue\n",
    "                        topic_name = refined_topics.get(topic_id, f\"Topic {topic_id}\")\n",
    "                        drama_topic_weights[topic_name] = count / total_count\n",
    "                    \n",
    "                    heatmap_data[drama_name].update(drama_topic_weights)\n",
    "                    \n",
    "                # Sentiment analysis for Violin Plot\n",
    "                sentiment_results = analyzer.analyze_sentiment(texts_in_period)\n",
    "                sentiment_scores = [res['score'] if res['label'] == 'positive' else -res['score'] for res in sentiment_results]\n",
    "                \n",
    "                violin_plot_data[period_name][category] = sentiment_scores\n",
    "        \n",
    "        # 生成熱力圖\n",
    "        if heatmap_data: \n",
    "            topic_mapping = topic_mapping_list.get(culture)\n",
    "            fig_heatmap = visualize_heatmap(heatmap_data, trickery_elements)\n",
    "            fig_heatmap.update_layout(title_text='權謀元素熱力圖', xaxis_title='權謀元素名稱', yaxis_title='作品名稱')\n",
    "            output_path = os.path.join(docs_dir, f\"drama_category_comparison/topic_heatmap/{culture}/all_generation.html\")\n",
    "            save_plotly_html(fig_heatmap,output_path)\n",
    "            print(f\"Topic heatmap report is saved.\")\n",
    "\n",
    "        # 生成小提琴圖\n",
    "        if violin_plot_data:\n",
    "            fig_violin_plot = visualize_four_quadrants_violin_plot(violin_plot_data)\n",
    "            fig_violin_plot.update_layout(\n",
    "                title_text='復仇劇與一般權謀劇情緒波動',\n",
    "                yaxis_title='情緒分數',\n",
    "                violinmode='group'\n",
    "            )\n",
    "            output_path = os.path.join(docs_dir, f\"drama_category_comparison/violin_plot/{culture}/all_generation.html\")\n",
    "            save_plotly_html(fig_violin_plot,output_path)\n",
    "            print(f\"Violin plot report is saved.\")\n",
    "\n",
    "generate_drama_category_reports(docs_dir, all_data_df, topic_mapping_list, cultures, time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc86da40-763f-4d32-a85a-95a248191c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating per-drama radar chart reports...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:10:27,871 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f524d4c9fb497585aa68b93e2e9051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:12:00,500 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 14:12:00,500 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 14:12:24,644 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:12:24,645 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 14:12:24,730 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:12:24,733 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\c-116\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.565 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2025-09-25 14:12:26,472 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing radar chart for 琅琊榜...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8003614b99304aaa9637ca6f5622c385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:13:08,167 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:13:15,773 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:13:15,773 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:13:15,822 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_analysis/radar_chart/zh/琅琊榜.html\n",
      "Radar chart for 琅琊榜 is saved.\n",
      "Processing radar chart for 慶餘年 第一季...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fbbfba2a3e4f68809b48c94e6b03ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:14:08,661 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:14:09,844 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:14:09,845 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:14:09,892 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_analysis/radar_chart/zh/慶餘年_第一季.html\n",
      "Radar chart for 慶餘年 第一季 is saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:14:13,516 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dece828bee4a2197b93e0790359ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:14:21,593 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-25 14:14:21,594 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-25 14:14:34,305 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:14:34,305 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-25 14:14:34,383 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-25 14:14:34,385 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-25 14:14:34,461 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "精煉主題清單:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing radar chart for Nirvana in Fire...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b3f15b9afb42fa8f73acfe2507d3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:14:38,264 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:14:40,066 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:14:40,066 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:14:40,103 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_analysis/radar_chart/en/Nirvana_in_Fire.html\n",
      "Radar chart for Nirvana in Fire is saved.\n",
      "Processing radar chart for Joy of Life Season 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa2e042de164113927fbe4a840e9971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:14:44,964 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-25 14:14:46,355 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-25 14:14:46,355 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-25 14:14:46,414 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已成功儲存至: C:\\My data\\0.change jobs\\data_science_practice\\etl_showcase\\application\\..\\docs/trickery_drama_evolution_study\\drama_analysis/radar_chart/en/Joy_of_Life_Season_1.html\n",
      "Radar chart for Joy of Life Season 1 is saved.\n"
     ]
    }
   ],
   "source": [
    "def generate_per_drama_reports(docs_dir, data, topic_mapping_list, cultures):\n",
    "    \"\"\"\n",
    "    生成各劇雷達圖報告。\n",
    "    \"\"\"\n",
    "    print(\"Generating per-drama radar chart reports...\")\n",
    "    \n",
    "    for culture in cultures:\n",
    "        df_filtered = data[data['Cultural sphere'] == culture]\n",
    "        dramas = df_filtered['Screenwork'].unique()\n",
    "        texts_culture = df_filtered['text'].tolist()\n",
    "        # 檢查文件數量是否足夠， UMAP 降維時樣本數量需大於預設值，否則會報錯\n",
    "        if len(texts_culture) < default_n_neighbors():\n",
    "            print(f\"BERTopic 建模所需文件數不足 (至少{default_n_neighbors()}個)，目前只有 {len(texts_culture)} 個。已跳過 {culture} 的主題報告生成。\")\n",
    "            continue        \n",
    "        topic_model = get_BERTopic_model(culture=culture)\n",
    "        _ = topic_model.fit_transform(texts_culture)\n",
    "        \n",
    "        # 精鍊主題清單\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        refined_topics = transform_to_refined_topics_by_culture(topic_info, topic_mapping_list, culture)\n",
    "        \n",
    "        for drama_name in dramas:\n",
    "            print(f\"Processing radar chart for {drama_name}...\")\n",
    "            df_filtered = data[\n",
    "                (data['Cultural sphere'] == culture) &\n",
    "                (data['Screenwork'] == drama_name)\n",
    "            ]\n",
    "            \n",
    "            if df_filtered.empty:\n",
    "                continue\n",
    "\n",
    "            drama_texts = df_filtered['text'].tolist()\n",
    "            drama_topics, _ = topic_model.transform(drama_texts)\n",
    "\n",
    "            # 根據文本數排序，選擇前10個主題，再計算選出來的主題的總文本數 \n",
    "            top_num = 10\n",
    "            topic_counts = pd.Series(drama_topics).value_counts()         \n",
    "            top_topics = topic_counts[topic_counts.index != -1].head(top_num)\n",
    "            total_count = top_topics.sum()\n",
    "\n",
    "            topic_proportions = defaultdict(float)\n",
    "            for topic_id, count in top_topics.items():\n",
    "                refined_name = refined_topics.get(topic_id, f\"Topic {topic_id}\")\n",
    "                topic_proportions[refined_name] += count / total_count\n",
    "\n",
    "            fig_radar = visualize_radar_chart(topic_proportions)\n",
    "            fig_radar.update_layout(\n",
    "                title=f'{drama_name} Top {top_num} 權謀元素'\n",
    "            )          \n",
    "            safe_drama_name = drama_name.replace(' ', '_').replace('?', '').replace(':', '_')\n",
    "            output_path = os.path.join(docs_dir, f\"drama_analysis/radar_chart/{culture}/{safe_drama_name}.html\")\n",
    "            save_plotly_html(fig_radar,output_path)\n",
    "            print(f\"Radar chart for {drama_name} is saved.\")\n",
    "generate_per_drama_reports(docs_dir, all_data_df, topic_mapping_list, cultures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139a444-d179-4a75-8d7e-8e9caa5aa51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
